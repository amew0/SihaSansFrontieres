{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1267593,"sourceType":"datasetVersion","datasetId":723383}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pennylane","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport nibabel as nib  # for loading NIfTI files\nimport os\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport pennylane as qml\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport h5py\n\n# Directory containing .h5 files\ndirectory = \"/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data\"\n\n# Create a list of all .h5 files in the directory\nh5_files = [f for f in os.listdir(directory) if f.endswith('.h5')]\nprint(f\"Found {len(h5_files)} .h5 files:\\nExample file names:{h5_files[:3]}\")\n\n# Open the first .h5 file in the list to inspect its contents\nif h5_files:\n    file_path = os.path.join(directory, h5_files[25070])\n    with h5py.File(file_path, 'r') as file:\n        print(\"\\nKeys for each file:\", list(file.keys()))\n        for key in file.keys():\n            print(f\"\\nData type of {key}:\", type(file[key][()]))\n            print(f\"Shape of {key}:\", file[key].shape)\n            print(f\"Array dtype: {file[key].dtype}\")\n            print(f\"Array max val: {np.max(file[key])}\")\n            print(f\"Array min val: {np.min(file[key])}\")\nelse:\n    print(\"No .h5 files found in the directory.\")","metadata":{"tags":[],"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Viewing Sample Images","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.rcParams['figure.facecolor'] = '#171717'\nplt.rcParams['text.color']       = '#DDDDDD'\n\ndef display_image_channels(image, title='Image Channels'):\n    channel_names = ['T1-weighted (T1)', 'T1-weighted post contrast (T1c)', 'T2-weighted (T2)', 'Fluid Attenuated Inversion Recovery (FLAIR)']\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n    for idx, ax in enumerate(axes.flatten()):\n        channel_image = image[idx, :, :]  # Transpose the array to display the channel\n        ax.imshow(channel_image, cmap='magma')\n        ax.axis('off')\n        ax.set_title(channel_names[idx])\n    plt.tight_layout()\n    plt.suptitle(title, fontsize=20, y=1.03)\n    plt.show()\n\ndef display_mask_channels_as_rgb(mask, title='Mask Channels as RGB', save=False):\n    channel_names = ['Necrotic (NEC)', 'Edema (ED)', 'Tumour (ET)']\n    fig, axes = plt.subplots(1, 3, figsize=(9.75, 5))\n    for idx, ax in enumerate(axes):\n        rgb_mask = np.zeros((mask.shape[1], mask.shape[2], 3), dtype=np.uint8)\n        rgb_mask[..., idx] = mask[idx, :, :] * 255  # Transpose the array to display the channel\n        ax.imshow(rgb_mask)\n        ax.axis('off')\n        ax.set_title(channel_names[idx])\n    plt.suptitle(title, fontsize=20, y=0.93)\n    plt.tight_layout()\n    if save:\n        plt.savefig(\"prediction.pdf\")\n    plt.show()\n\ndef overlay_masks_on_image(image, mask, title='Brain MRI with Tumour Masks Overlay'):\n    t1_image = image[0, :, :]  # Use the first channel of the image\n    t1_image_normalized = (t1_image - t1_image.min()) / (t1_image.max() - t1_image.min())\n\n    rgb_image = np.stack([t1_image_normalized] * 3, axis=-1)\n    color_mask = np.stack([mask[0, :, :], mask[1, :, :], mask[2, :, :]], axis=-1)\n    rgb_image = np.where(color_mask, color_mask, rgb_image)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(rgb_image)\n    plt.title(title, fontsize=18, y=1.02)\n    plt.axis('off')\n    plt.show()\n    \n    \n# Sample image to view\nsample_file_path = os.path.join(directory, h5_files[25070])\ndata = {}\nwith h5py.File(sample_file_path, 'r') as file:\n    for key in file.keys():\n        data[key] = file[key][()]\n\n# Transpose the image and mask to have channels first\nimage = data['image'].transpose(2, 0, 1)\nmask = data['mask'].transpose(2, 0, 1)\n\n# View images using plotting functions\ndisplay_image_channels(image)\ndisplay_mask_channels_as_rgb(mask)\noverlay_masks_on_image(image, mask)","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating a Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass BrainScanDataset(Dataset):\n    def __init__(self, file_paths, deterministic=False):\n        self.file_paths = file_paths\n        if deterministic:  # To always generate the same test images for consistency\n            np.random.seed(1)\n        np.random.shuffle(self.file_paths)\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        # Load h5 file, get image and mask\n        file_path = self.file_paths[idx]\n        with h5py.File(file_path, 'r') as file:\n            image = file['image'][()]\n            mask = file['mask'][()]\n            \n            # Reshape: (H, W, C) -> (C, H, W)\n            image = image.transpose((2, 0, 1))\n            mask = mask.transpose((2, 0, 1))\n            \n            # Adjusting pixel values for each channel in the image so they are between 0 and 255\n            for i in range(image.shape[0]):    # Iterate over channels\n                min_val = np.min(image[i])     # Find the min value in the channel\n                image[i] = image[i] - min_val  # Shift values to ensure min is 0\n                max_val = np.max(image[i]) + 1e-4     # Find max value to scale max to 1 now.\n                image[i] = image[i] / max_val\n            \n            # Convert to float and scale the whole image\n            image = torch.tensor(image, dtype=torch.float32)\n            mask = torch.tensor(mask, dtype=torch.float32) \n            \n        return image, mask\n\n# Build .h5 file paths from directory containing .h5 files\nh5_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.h5')]\nnp.random.seed(42)\nnp.random.shuffle(h5_files)\n\n# Split the dataset into train and validation sets (90:10)\nsplit_idx = int(0.9 * len(h5_files))\ntrain_files = h5_files[:split_idx]\nval_files = h5_files[split_idx:]\n\n# Create the train and val datasets\ntrain_dataset = BrainScanDataset(train_files)\nval_dataset = BrainScanDataset(val_files, deterministic=True)\n\n# Sample dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n\n# Use this to generate test images to view later\ntest_input_iterator = iter(DataLoader(val_dataset, batch_size=1, shuffle=False))\n\n# Verifying dataloaders work\nfor images, masks in train_dataloader:\n    print(\"Training batch - Images shape:\", images.shape, \"Masks shape:\", masks.shape)\n    break\nfor images, masks in val_dataloader:\n    print(\"Validation batch - Images shape:\", images.shape, \"Masks shape:\", masks.shape)\n    break","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nclass QNN(nn.Module):\n    def __init__(self, num_qubits, num_layers, q_device: str = \"lightning.qubit\"):\n        super().__init__()\n\n        #classical device to train on (gpu/cpu)\n        #self.c_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.c_device = \"cpu\"\n\n        #init quantum device \n        self.num_qubits = num_qubits\n        self.q_device = qml.device(q_device, wires=self.num_qubits)\n\n        #number of quantum layers\n        self.num_layers = num_layers\n\n        #quantum circuit\n        @qml.qnode(device=self.q_device, interface='torch')\n        def circuit(inputs, weights):\n            # Encoding of input values\n            for j in range(self.num_qubits):\n                qml.RX(np.pi*inputs[j], wires=j)\n            #qml.Barrier(wires=range(self.num_qubits))\n            #layers of QNN  \n            for l in range(self.num_layers):\n                 # Variational quantum layers\n                for j in range(self.num_qubits):\n                    qml.RY(weights[2*l*self.num_qubits+j], wires=j)\n                # Variational quantum layers\n                for j in range(self.num_qubits):\n                    qml.RZ(weights[(2*l+1)*self.num_qubits+j], wires=j)\n\n              # Entangling layer\n                for j in range(self.num_qubits - 1):\n                    qml.CNOT(wires=[j, j + 1])\n                qml.CNOT(wires=[self.num_qubits - 1, 0])\n\n            # Measurements\n            return [qml.expval(qml.PauliZ(j)) for j in range(self.num_qubits)]\n    \n        weight_shapes = {\"weights\": 2*self.num_qubits*self.num_layers}\n        self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)\n        self.qlayer = self.qlayer.to(self.c_device)\n\n    def forward(self, inputs):\n        output = torch.zeros(inputs.size()).to(self.c_device)\n        for i, x in enumerate(inputs):\n            output[i] = self.qlayer(x)\n        return output\n    \n    def draw(self):\n        # build circuit by sending dummy data through it\n        qml.drawer.use_style('pennylane_sketch')\n        qml.draw_mpl(self.qlayer)(torch.from_numpy(np.zeros(self.num_qubits)))\n        self.qlayer.zero_grad()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, activation=nn.ReLU()):\n        super().__init__()\n        self.encoder_block = nn.Sequential(\n            nn.Conv2d(in_channels,  out_channels, kernel_size=(3,3), stride=1, padding=1),\n            activation,\n            nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), stride=1, padding=1),\n            activation\n        )\n    def forward(self, x):\n        return self.encoder_block(x)\n    \nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, activation=nn.ReLU()):\n        super().__init__()\n        self.decoder_block = nn.Sequential(\n            nn.Conv2d(in_channels,  in_channels//2, kernel_size=(3,3), stride=1, padding=1),\n            activation,\n            nn.Conv2d(in_channels//2, out_channels, kernel_size=(3,3), stride=1, padding=1),\n            activation\n        )\n    def forward(self, x):\n        return self.decoder_block(x)\n\nclass HQNN(nn.Module):\n    def __init__(self, num_qubits, num_layers):\n        super().__init__()\n        # Config\n        self.num_qubits = num_qubits\n        self.num_layers = num_layers\n        \n        \n        # Config\n        in_channels  = 4   # Input images have 4 channels\n        out_channels = 3   # Mask has 3 channels\n        n_filters    = 32  # Scaled down from 64 in original paper\n        activation   = nn.ReLU()\n        \n        # Up and downsampling methods\n        self.downsample  = nn.MaxPool2d((2,2), stride=2)\n        self.upsample    = nn.UpsamplingBilinear2d(scale_factor=2)\n        \n        # Encoder\n        self.enc_block_1 = EncoderBlock(in_channels, 1*n_filters, activation)\n        self.enc_block_2 = EncoderBlock(1*n_filters, 2*n_filters, activation)\n        \n        \n#         self.enc_block_3 = EncoderBlock(2*n_filters, 4*n_filters, activation)\n#         self.enc_block_4 = EncoderBlock(4*n_filters, 8*n_filters, activation)\n        \n        # Bottleneck\n        dim = 57600 #dim of flattened data \n        self.flatten = nn.Flatten(), \n        self.linear1 = nn.Linear(dim, self.num_qubits)\n        self.bottleneck = QNN(self.num_qubits, self.num_layers)\n        self.linear2 = nn.Linear(self.num_qubits, dim)\n        \n        # Decoder\n#         self.dec_block_4 = DecoderBlock(16*n_filters, 4*n_filters, activation)\n#         self.dec_block_3 = DecoderBlock( 8*n_filters, 2*n_filters, activation)\n        self.dec_block_2 = DecoderBlock( 4*n_filters, 1*n_filters, activation)\n        self.dec_block_1 = DecoderBlock( 2*n_filters, 1*n_filters, activation)\n        \n        # Output projection\n        self.output      = nn.Conv2d(1*n_filters,  out_channels, kernel_size=(1,1), stride=1, padding=0)\n\n        \n    def forward(self, x):\n                # Encoder\n        skip_1 = self.enc_block_1(x)\n        x      = self.downsample(skip_1)\n        skip_2 = self.enc_block_2(x)\n        x      = self.downsample(skip_2)\n#         skip_3 = self.enc_block_3(x)\n#         x      = self.downsample(skip_3)\n#         skip_4 = self.enc_block_4(x)\n#         x      = self.downsample(skip_4)\n        \n        shape = x.shape\n        # Bottleneck\n        x      = x.view(x.size(0), -1)\n        x      = self.linear1(x)\n        x      = self.bottleneck(x)\n        x      = self.linear2(x)\n\n        # Decoder\n        x      = x.reshape(shape)\n        \n        # Decoder\n#         x      = self.upsample(x)\n#         x      = torch.cat((x, skip_4), axis=1)  # Skip connection\n#         x      = self.dec_block_4(x)\n#         x      = self.upsample(x)\n#         x      = torch.cat((x, skip_3), axis=1)  # Skip connection\n#         x      = self.dec_block_3(x)\n        x      = self.upsample(x)\n        x      = torch.cat((x, skip_2), axis=1)  # Skip connection\n        x      = self.dec_block_2(x)\n        x      = self.upsample(x)\n        x      = torch.cat((x, skip_1), axis=1)  # Skip connection\n        x      = self.dec_block_1(x)\n        x      = self.output(x)\n        return x\n    \n# Function to count number of parameters in a model for comparisons later\ndef count_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f'Total Parameters: {total_params:,}\\n')\n\n# Function that saves a model to specified path\ndef save_model(model, path='model_weights.pth'):\n    torch.save(model.state_dict(), path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop\nThe code in the cell below defines and executes the training loop for our UNet models. \n\nThe training process involves:\n- Setting up the model and training configuration.\n- Initialising the model and moving it to the chosen device.\n- Adam optimizer for parameter updates and binary cross-entropy loss as the loss function.\n- We then iterate through each epoch, we can optionally dynamically adjust the learning rate.\n\nFor each epoch, we conduct both training and validation steps:\n- In the training step, we feed batches of data to the model, computes the loss, and updates the model parameters.\n- In the validation step, we evaluate the model on a separate dataset to monitor performance without updating model parameters.\n- We track and print out the average training and validation losses for each epoch, providing insights into the model's learning progress.\n\nFinally, the function returns lists of training and validation losses for further analysis or plotting.","metadata":{}},{"cell_type":"code","source":"def iou_score(outputs, labels):\n    outputs = torch.sigmoid(outputs) > 0.5  # Convert predictions to boolean tensor\n    labels = labels > 0.5  # Ensure labels are also boolean if they are not already\n    intersection = (outputs & labels).float().sum((1, 2))  # Sum over each image\n    union = (outputs | labels).float().sum((1, 2))\n    iou = (intersection + 1e-6) / (union + 1e-6)  # avoid division by zero by adding a small constant\n    return iou.mean()  # Return the mean IoU score for the batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader, config, verbose=True):\n    device = config['device']\n    n_epochs = config['n_epochs']\n    learning_rate = config['learning_rate']\n    batches_per_epoch = config['batches_per_epoch']\n    lr_decay_factor = config['lr_decay_factor']\n\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    train_epoch_losses = []\n    train_epoch_metrics = []\n    \n    val_epoch_losses = []\n    val_epoch_metrics = []\n    \n    print(\"Training...\")\n    for epoch in range(1, n_epochs + 1):\n        # Decay learning rate\n        current_lr = learning_rate * (lr_decay_factor ** (epoch - 1))\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n\n        # Training step\n        model.train()\n        train_epoch_loss = 0\n        train_epoch_metric = 0\n        for train_batch_idx, (train_inputs, train_targets) in enumerate(train_dataloader, start=1):\n            #if verbose: print(f\"\\rTrain batch: {train_batch_idx}/{batches_per_epoch}, Avg batch loss: {train_epoch_loss/train_batch_idx:.6f}\", end='')\n            #if verbose: print(f\"\\rTrain batch: {train_batch_idx}/{batches_per_epoch}, Avg batch loss: {train_epoch_metric/train_batch_idx:.6f}\", end='')\n\n            train_inputs = train_inputs.to(device)\n            train_targets = train_targets.to(device)\n            train_preds = model(train_inputs)\n            \n            train_batch_loss = loss_fn(train_preds, train_targets)\n            train_batch_metric = iou_score(train_preds, train_targets)\n            \n            train_epoch_loss += train_batch_loss.item()\n            train_epoch_metric += train_batch_metric\n\n            optimizer.zero_grad()\n            train_batch_loss.backward()\n            optimizer.step()\n\n            if train_batch_idx >= batches_per_epoch:\n                if verbose: print()\n                break\n        train_epoch_losses.append(train_epoch_loss)\n        train_epoch_metrics.append(train_epoch_metric/batches_per_epoch)\n\n        # Val step\n        model.eval()\n        val_epoch_loss = 0\n        val_epoch_metric = 0\n        with torch.no_grad():\n            for val_batch_idx, (val_inputs, val_targets) in enumerate(val_dataloader, start=1):\n                #if verbose: print(f\"\\rVal batch: {val_batch_idx}/{batches_per_epoch}, Avg batch loss: {val_epoch_loss/val_batch_idx:.6f}\", end='')\n                val_inputs = val_inputs.to(device)\n                val_targets = val_targets.to(device)\n                val_preds = model(val_inputs)\n                \n                val_batch_loss = loss_fn(val_preds, val_targets)\n                val_batch_metric = iou_score(val_preds, val_targets)\n                \n                val_epoch_loss += val_batch_loss.item()\n                val_epoch_metric += val_batch_metric.item()\n                \n                if val_batch_idx >= batches_per_epoch:\n                    if verbose: print()\n                    break\n        val_epoch_losses.append(val_epoch_loss)\n        val_epoch_metrics.append(val_epoch_metric/batches_per_epoch)\n\n        #if verbose: print(f\"Epoch: {epoch}, Train loss: {train_epoch_loss:.6f}, Val loss: {val_epoch_loss:.6f}, lr {current_lr:.6f}\\n\")\n        if verbose: print(f\"Epoch: {epoch}, Train metric: {train_epoch_metric/batches_per_epoch:.6f}, Val metric: {val_epoch_metric/batches_per_epoch:.6f}\\n\")\n        \n    print(\"Training complete.\")\n    return train_epoch_metrics, val_epoch_metrics, train_epoch_losses, val_epoch_losses","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \n# Settings for training\ntrain_config = {\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'n_epochs':          20,\n    'batch_size':        20,\n    'learning_rate':     1e-3,\n    'batches_per_epoch': 10,\n    'lr_decay_factor':   1\n}\n\n# Create UNet model and count params\nmodel = HQNN(4, 1)\ncount_parameters(model)\n\n# Create dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=train_config['batch_size'], shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=train_config['batch_size'], shuffle=False)\n\n# Train model\ntrain_epoch_metrics, val_epoch_metrics, _, _ = train_model(model, train_dataloader, val_dataloader, train_config, verbose=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning Curves\nHere we visualise the training and validation loss over each epoch, helping assess the model's learning progress and identify overfitting.","metadata":{}},{"cell_type":"code","source":"def plot_learning_curves(train_epoch_losses, val_epoch_losses):\n    plt.style.use('ggplot')\n    plt.rcParams['text.color'] = '#333333'\n\n    fig, axis = plt.subplots(1, 1, figsize=(10, 6))\n\n    # Plot training and validation loss (NaN is used to offset epochs by 1)\n    axis.plot([np.NaN] + train_epoch_losses, color='#636EFA', marker='o', linestyle='-', linewidth=2, markersize=5, label='Training Loss')\n    axis.plot([np.NaN] + val_epoch_losses,   color='#EFA363', marker='s', linestyle='-', linewidth=2, markersize=5, label='Validation Loss')\n\n    # Adding title, labels and formatting\n    axis.set_title('Training and Validation Loss Over Epochs', fontsize=16)\n    axis.set_xlabel('Epoch', fontsize=14)\n    axis.set_ylabel('Loss', fontsize=14)\n\n    axis.set_ylim(0, 1)\n    \n    axis.legend(fontsize=12)\n    axis.grid(True, which='both', linestyle='--', linewidth=0.5)\n    plt.savefig(\"learning_curve.pdf\")\n    plt.show()\n    \nplot_learning_curves( val_epoch_metrics, train_epoch_metrics)","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Viewing a Sample Prediction\nThis code demonstrates the model's performance on a test sample by displaying the input brain MRI images, the model's predicted mask, and the actual mask (ground truth).","metadata":{}},{"cell_type":"code","source":"def display_test_sample(model, test_input, test_target, device):\n    test_input, test_target = test_input.to(device), test_target.to(device)\n\n    # Obtain the model's prediction\n    test_pred = torch.sigmoid(model(test_input))\n\n    # Process the image and masks for visualization\n    image = test_input.detach().cpu().numpy().squeeze(0)\n    mask_pred = test_pred.detach().cpu().numpy().squeeze(0)\n    mask_target = test_target.detach().cpu().numpy().squeeze(0)\n\n    # Set the plot aesthetics\n    plt.rcParams['figure.facecolor'] = '#171717'\n    plt.rcParams['text.color']       = '#DDDDDD'\n\n    # Display the input image, predicted mask, and target mask\n    display_image_channels(image)\n    display_mask_channels_as_rgb(mask_pred, title='Predicted Mask Channels as RGB', save=True)\n    display_mask_channels_as_rgb(mask_target, title='Ground Truth as RGB')\n    \n\n# Set so model and these images are on the same device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n# Get an image from the validation dataset that the model hasn't been trained on\ntest_input, test_target = next(test_input_iterator)\n\ndisplay_test_sample(model, test_input, test_target, device)","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}