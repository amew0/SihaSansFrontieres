{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1267593,"sourceType":"datasetVersion","datasetId":723383}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport h5py\n\n# Directory containing .h5 files\ndirectory = \"/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data\"\n\n# Create a list of all .h5 files in the directory\nh5_files = [f for f in os.listdir(directory) if f.endswith('.h5')]\nprint(f\"Found {len(h5_files)} .h5 files:\\nExample file names:{h5_files[:3]}\")\n\n# Open the first .h5 file in the list to inspect its contents\nif h5_files:\n    file_path = os.path.join(directory, h5_files[25070])\n    with h5py.File(file_path, 'r') as file:\n        print(\"\\nKeys for each file:\", list(file.keys()))\n        for key in file.keys():\n            print(f\"\\nData type of {key}:\", type(file[key][()]))\n            print(f\"Shape of {key}:\", file[key].shape)\n            print(f\"Array dtype: {file[key].dtype}\")\n            print(f\"Array max val: {np.max(file[key])}\")\n            print(f\"Array min val: {np.min(file[key])}\")\nelse:\n    print(\"No .h5 files found in the directory.\")","metadata":{"tags":[],"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## About the Dataset\nOur dataset consists of images from brain scans, as well as masked labels indicating regions of abnormal tissue. Each image has 4 channels.\n\n#### Image channels:\n1) **T1-weighted (T1)**: This sequence produces a high resolution image of the brain's anatomy. It's good for visualising the structure of the brain but not as sensitive to tumour tissue as other sequences.\n\n2) **T1-weighted post contrast (T1c or T1Gd)**: After the injection of a contrast agent (usually gadolinium), T1-weighted images are taken again. The contrast agent enhances areas with a high degree of vascularity and blood-brain barrier breakdown, which is typical in tumour tissue, making this sequence particularly useful for highlighting malignant tumours.\n\n3) **T2-weighted (T2)**: T2 images provide excellent contrast of the brain's fluid spaces and are sensitive to edema (swelling), which often surrounds tumours. It helps in visualizing both the tumour and changes in nearby tissue.\n\n4) **Fluid Attenuated Inversion Recovery (FLAIR)**: This sequence suppresses the fluid signal, making it easier to see peritumoral edema (swelling around the tumour) and differentiate it from the cerebrospinal fluid. It's particularly useful for identifying lesions near or within the ventricles.\n\nRegarding the masks, they represent segmentations, containing areas of interest within the brain scans, specifically focusing on abnormal tissue related to brain tumours. We will use these masks for training models in segmenting brain tumours from normal brain tissue. Each mask has 3 channels.\n\n#### Mask channels\n1) **Necrotic and Non-Enhancing Tumour Core (NCR/NET)**: This masks out the necrotic (dead) part of the tumour, which doesn't enhance with contrast agent, and the non-enhancing tumour core.\n\n2) **Edema (ED)**: This channel masks out the edema, the swelling or accumulation of fluid around the tumour.\n\n3) **Enhancing Tumour (ET)**: This masks out the enhancing tumour, which is the region of the tumour that shows uptake of contrast material and is often considered the most aggressive part of the tumour.\n\n### Viewing Sample Images\nWe'll use matplotlib imshow to view some example brain scans, and see the tumour locations.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.rcParams['figure.facecolor'] = '#171717'\nplt.rcParams['text.color']       = '#DDDDDD'\n\ndef display_image_channels(image, title='Image Channels', save=False, name=\"*\"):\n    channel_names = ['T1-weighted (T1)', 'T1-weighted post contrast (T1c)', 'T2-weighted (T2)', 'Fluid Attenuated Inversion Recovery (FLAIR)']\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n    for idx, ax in enumerate(axes.flatten()):\n        channel_image = image[idx, :, :]  # Transpose the array to display the channel\n        ax.imshow(channel_image, cmap='magma')\n        ax.axis('off')\n        ax.set_title(channel_names[idx])\n    plt.tight_layout()\n    plt.suptitle(title, fontsize=20, y=1.03)\n    if save:\n        plt.savefig(name+'.pdf')\n    plt.show()\n\ndef display_mask_channels_as_rgb(mask, title='Mask Channels as RGB', save=False, name=\"*\"):\n    channel_names = ['Necrotic (NEC)', 'Edema (ED)', 'Tumour (ET)']\n    #fig, axes = plt.subplots(1, 3, figsize=(9.75, 5))\n    fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n    for idx, ax in enumerate([axes]):\n        idx += 1\n        rgb_mask = np.zeros((mask.shape[1], mask.shape[2], 3), dtype=np.uint8)\n        rgb_mask[..., idx] = mask[idx, :, :] * 255  # Transpose the array to display the channel\n        ax.imshow(rgb_mask)\n        ax.axis('off')\n        ax.set_title(channel_names[idx])\n    plt.suptitle(title, fontsize=20, y=0.93)\n    plt.tight_layout()\n    if save:\n        fig.savefig(f'{name}.png')\n    plt.show()\n\ndef overlay_masks_on_image(image, mask, title='Brain MRI with Tumour Masks Overlay'):\n    t1_image = image[0, :, :]  # Use the first channel of the image\n    t1_image_normalized = (t1_image - t1_image.min()) / (t1_image.max() - t1_image.min())\n\n    rgb_image = np.stack([t1_image_normalized] * 3, axis=-1)\n    color_mask = np.stack([mask[0, :, :], mask[1, :, :], mask[2, :, :]], axis=-1)\n    rgb_image = np.where(color_mask, color_mask, rgb_image)\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(rgb_image)\n    plt.title(title, fontsize=18, y=1.02)\n    plt.axis('off')\n    plt.show()\n    \n    \n# Sample image to view\nsample_file_path = os.path.join(directory, h5_files[25070])\ndata = {}\nwith h5py.File(sample_file_path, 'r') as file:\n    for key in file.keys():\n        data[key] = file[key][()]\n\n# Transpose the image and mask to have channels first\nimage = data['image'].transpose(2, 0, 1)\nmask = data['mask'].transpose(2, 0, 1)\n\n# View images using plotting functions\ndisplay_image_channels(image)\ndisplay_mask_channels_as_rgb(mask)\noverlay_masks_on_image(image, mask)","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating a Dataset and DataLoader\nOur dataset is fairly large at ~8GB, so during training we will use dataloaders to load batches of image + target pairs. Since these images are formatted unconventionally, we'll need to build a custom Dataset object and DataLoader. \n\nImages are stored using the `float64` data type with pixel intensities ranging from ~-0.5 to ~230 and `height`, `width`, `channels` format. The masks use the `uint8` data type and pixels have values of either 0 or 1. \n\nOur dataloader will need to load an image, reshape it so it's formatted `channels`, `height`, `width` for use in PyTorch, adjust pixel intensities so the minimum value is 0 on every image, then scale each channel so the max is 1, then convert to a `torch.float32` tensor.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass BrainScanDataset(Dataset):\n    def __init__(self, file_paths, deterministic=False):\n        self.file_paths = file_paths\n        if deterministic:  # To always generate the same test images for consistency\n            np.random.seed(1)\n        np.random.shuffle(self.file_paths)\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        # Load h5 file, get image and mask\n        file_path = self.file_paths[idx]\n        with h5py.File(file_path, 'r') as file:\n            image = file['image'][()]\n            mask = file['mask'][()]\n            \n            # Reshape: (H, W, C) -> (C, H, W)\n            image = image.transpose((2, 0, 1))\n            mask = mask.transpose((2, 0, 1))\n            \n            # Adjusting pixel values for each channel in the image so they are between 0 and 255\n            for i in range(image.shape[0]):    # Iterate over channels\n                min_val = np.min(image[i])     # Find the min value in the channel\n                image[i] = image[i] - min_val  # Shift values to ensure min is 0\n                max_val = np.max(image[i]) + 1e-4     # Find max value to scale max to 1 now.\n                image[i] = image[i] / max_val\n            \n            # Convert to float and scale the whole image\n            image = torch.tensor(image, dtype=torch.float32)\n            mask = torch.tensor(mask, dtype=torch.float32) \n            \n        return image, mask\n\n# Build .h5 file paths from directory containing .h5 files\nh5_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.h5')]\nnp.random.seed(42)\nnp.random.shuffle(h5_files)\n\n# Split the dataset into train and validation sets (90:10)\nsplit_idx = int(0.9 * len(h5_files))\ntrain_files = h5_files[:split_idx]\nval_files = h5_files[split_idx:]\n\n# Create the train and val datasets\ntrain_dataset = BrainScanDataset(train_files)\nval_dataset = BrainScanDataset(val_files, deterministic=True)\n\n# Sample dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n\n# Use this to generate test images to view later\ntest_input_iterator = iter(DataLoader(val_dataset, batch_size=1, shuffle=False))\n\n# Verifying dataloaders work\nfor images, masks in train_dataloader:\n    print(\"Training batch - Images shape:\", images.shape, \"Masks shape:\", masks.shape)\n    break\n    \nfor images, masks in val_dataloader:\n    print(\"Validation batch - Images shape:\", images.shape, \"Masks shape:\", masks.shape)\n    break","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The UNet Architecture\n\n<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" \n     align=\"right\" \n     width=\"500\"\n     style=\"padding: 20px;\" />\n     \n[UNet](https://arxiv.org/abs/1505.04597) is a deep learning neural network architecture originally designed for image segmentation tasks, but has since found use in GANs and latent diffusion models. Similar to autoencoders, the UNet structures include an encoder for compressing the input into a lower dimensional representation and a decoder for reconstructing the output from this compressed form. However, UNet distinguishes itself from autoencoders with several features:\n\n- **Bottleneck**: At the centre of UNet is the bottleneck layer, situated between the encoder and decoder. This component captures the most abstract representation of the input.\n\n- **Symmetrical Design**: The encoder and decoder parts of UNet are mirror images of each other. This symmetry facilitates the implementation of skip connections, which directly connect layers of the encoder to their corresponding layers in the decoder.\n\n- **Skip Connections**: Unlike conventional autoencoders, UNet incorporates skip connections that can bypass the bottleneck, directly connecting the encoder to the decoder. These connections transfer contextual information from the spatially rich encoder, to the more feature heavy decoder, aiding in the precise localisation necessary for detailed segmentation.\n\nIn this project we're going to start off with the original UNet architecture, then work on building in some staple CV tweaks like separable convolution layers, batch norm, fewer activation functions, additive skip connections, larger kernels, then finally include the attention mechanism from [Attention UNet](https://arxiv.org/abs/1804.03999).","metadata":{}},{"cell_type":"code","source":"def iou_score(outputs, labels):\n    outputs = torch.sigmoid(outputs) > 0.5  # Convert predictions to boolean tensor\n    labels = labels > 0.5  # Ensure labels are also boolean if they are not already\n    intersection = (outputs & labels).float().sum((1, 2))  # Sum over each image\n    union = (outputs | labels).float().sum((1, 2))\n    iou = (intersection + 1e-6) / (union + 1e-6)  # avoid division by zero by adding a small constant\n    return iou.mean()  # Return the mean IoU score for the batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, activation=nn.ReLU()):\n        super().__init__()\n        self.encoder_block = nn.Sequential(\n            nn.Conv2d(in_channels,  out_channels, kernel_size=(3,3), stride=1, padding=1),\n            activation,\n            nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), stride=1, padding=1),\n            activation\n        )\n    def forward(self, x):\n        return self.encoder_block(x)\n    \nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, activation=nn.ReLU()):\n        super().__init__()\n        self.decoder_block = nn.Sequential(\n            nn.Conv2d(in_channels,  in_channels//2, kernel_size=(3,3), stride=1, padding=1),\n            activation,\n            nn.Conv2d(in_channels//2, out_channels, kernel_size=(3,3), stride=1, padding=1),\n            activation\n        )\n    def forward(self, x):\n        return self.decoder_block(x)\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Config\n        in_channels  = 4   # Input images have 4 channels\n        out_channels = 3   # Mask has 3 channels\n        n_filters    = 32  # Scaled down from 64 in original paper\n        activation   = nn.ReLU()\n        \n        # Up and downsampling methods\n        self.downsample  = nn.MaxPool2d((2,2), stride=2)\n        self.upsample    = nn.UpsamplingBilinear2d(scale_factor=2)\n        \n        # Encoder\n        self.enc_block_1 = EncoderBlock(in_channels, 1*n_filters, activation)\n        self.enc_block_2 = EncoderBlock(1*n_filters, 2*n_filters, activation)\n        self.enc_block_3 = EncoderBlock(2*n_filters, 4*n_filters, activation)\n        self.enc_block_4 = EncoderBlock(4*n_filters, 8*n_filters, activation)\n        \n        # Bottleneck\n        self.bottleneck  = nn.Sequential(\n            nn.Conv2d( 8*n_filters, 16*n_filters, kernel_size=(3,3), stride=1, padding=1),\n            activation,\n            nn.Conv2d(16*n_filters,  8*n_filters, kernel_size=(3,3), stride=1, padding=1),\n            activation\n        )\n        \n        # Decoder\n        self.dec_block_4 = DecoderBlock(16*n_filters, 4*n_filters, activation)\n        self.dec_block_3 = DecoderBlock( 8*n_filters, 2*n_filters, activation)\n        self.dec_block_2 = DecoderBlock( 4*n_filters, 1*n_filters, activation)\n        self.dec_block_1 = DecoderBlock( 2*n_filters, 1*n_filters, activation)\n        \n        # Output projection\n        self.output      = nn.Conv2d(1*n_filters,  out_channels, kernel_size=(1,1), stride=1, padding=0)\n\n        \n    def forward(self, x):\n        # Encoder\n        skip_1 = self.enc_block_1(x)\n        x      = self.downsample(skip_1)\n        skip_2 = self.enc_block_2(x)\n        x      = self.downsample(skip_2)\n        skip_3 = self.enc_block_3(x)\n        x      = self.downsample(skip_3)\n        skip_4 = self.enc_block_4(x)\n        x      = self.downsample(skip_4)\n        \n        # Bottleneck\n        x      = self.bottleneck(x)\n        \n        # Decoder\n        x      = self.upsample(x)\n        x      = torch.cat((x, skip_4), axis=1)  # Skip connection\n        x      = self.dec_block_4(x)\n        x      = self.upsample(x)\n        x      = torch.cat((x, skip_3), axis=1)  # Skip connection\n        x      = self.dec_block_3(x)\n        x      = self.upsample(x)\n        x      = torch.cat((x, skip_2), axis=1)  # Skip connection\n        x      = self.dec_block_2(x)\n        x      = self.upsample(x)\n        x      = torch.cat((x, skip_1), axis=1)  # Skip connection\n        x      = self.dec_block_1(x)\n        x      = self.output(x)\n        return x\n    \n# Function to count number of parameters in a model for comparisons later\ndef count_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f'Total Parameters: {total_params:,}\\n')\n\n# Function that saves a model to specified path\ndef save_model(model, path='model_weights.pth'):\n    torch.save(model.state_dict(), path)","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop\nThe code in the cell below defines and executes the training loop for our UNet models. \n\nThe training process involves:\n- Setting up the model and training configuration.\n- Initialising the model and moving it to the chosen device.\n- Adam optimizer for parameter updates and binary cross-entropy loss as the loss function.\n- We then iterate through each epoch, we can optionally dynamically adjust the learning rate.\n\nFor each epoch, we conduct both training and validation steps:\n- In the training step, we feed batches of data to the model, computes the loss, and updates the model parameters.\n- In the validation step, we evaluate the model on a separate dataset to monitor performance without updating model parameters.\n- We track and print out the average training and validation losses for each epoch, providing insights into the model's learning progress.\n\nFinally, the function returns lists of training and validation losses for further analysis or plotting.","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader, config, verbose=True):\n    device = config['device']\n    n_epochs = config['n_epochs']\n    learning_rate = config['learning_rate']\n    batches_per_epoch = config['batches_per_epoch']\n    lr_decay_factor = config['lr_decay_factor']\n\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    train_epoch_losses = []\n    train_epoch_metrics = []\n    \n    val_epoch_losses = []\n    val_epoch_metrics = []\n    \n    print(\"Training...\")\n    for epoch in range(1, n_epochs + 1):\n        # Decay learning rate\n        current_lr = learning_rate * (lr_decay_factor ** (epoch - 1))\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n\n        # Training step\n        model.train()\n        train_epoch_loss = 0\n        train_epoch_metric = 0\n        for train_batch_idx, (train_inputs, train_targets) in enumerate(train_dataloader, start=1):\n            #if verbose: print(f\"\\rTrain batch: {train_batch_idx}/{batches_per_epoch}, Avg batch loss: {train_epoch_loss/train_batch_idx:.6f}\", end='')\n            #if verbose: print(f\"\\rTrain batch: {train_batch_idx}/{batches_per_epoch}, Avg batch loss: {train_epoch_metric/train_batch_idx:.6f}\", end='')\n\n            train_inputs = train_inputs.to(device)\n            train_targets = train_targets.to(device)\n            train_preds = model(train_inputs)\n            \n            train_batch_loss = loss_fn(train_preds, train_targets)\n            train_batch_metric = iou_score(train_preds, train_targets)\n            \n            train_epoch_loss += train_batch_loss.item()\n            train_epoch_metric += train_batch_metric.item()\n            optimizer.zero_grad()\n            train_batch_loss.backward()\n            optimizer.step()\n\n            if train_batch_idx >= batches_per_epoch:\n                if verbose: print()\n                break\n        train_epoch_losses.append(train_epoch_loss)\n        train_epoch_metrics.append(train_epoch_metric/batches_per_epoch)\n\n        # Val step\n        model.eval()\n        val_epoch_loss = 0\n        val_epoch_metric = 0\n        with torch.no_grad():\n            for val_batch_idx, (val_inputs, val_targets) in enumerate(val_dataloader, start=1):\n                #if verbose: print(f\"\\rVal batch: {val_batch_idx}/{batches_per_epoch}, Avg batch loss: {val_epoch_loss/val_batch_idx:.6f}\", end='')\n                val_inputs = val_inputs.to(device)\n                val_targets = val_targets.to(device)\n                val_preds = model(val_inputs)\n                \n                val_batch_loss = loss_fn(val_preds, val_targets)\n                val_batch_metric = iou_score(val_preds, val_targets)\n                \n                val_epoch_loss += val_batch_loss.item()\n                val_epoch_metric += val_batch_metric.item()\n                \n                if val_batch_idx >= batches_per_epoch:\n                    if verbose: print()\n                    break\n        val_epoch_losses.append(val_epoch_loss)\n        val_epoch_metrics.append(val_epoch_metric/batches_per_epoch)\n\n        #if verbose: print(f\"Epoch: {epoch}, Train loss: {train_epoch_loss:.6f}, Val loss: {val_epoch_loss:.6f}, lr {current_lr:.6f}\\n\")\n        if verbose: print(f\"Epoch: {epoch}, Train metric: {train_epoch_metric/batches_per_epoch:.6f}, Val metric: {val_epoch_metric/batches_per_epoch:.6f}\\n\")\n        \n    print(\"Training complete.\")\n    return train_epoch_metrics, val_epoch_metrics, train_epoch_losses, val_epoch_losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Settings for training\ntrain_config = {\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'n_epochs':          20,\n    'batch_size':        20,\n    'learning_rate':     1e-3,\n    'batches_per_epoch': 50,\n    'lr_decay_factor':   1\n}\n\n# Create UNet model and count params\nmodel = UNet()\ncount_parameters(model)\n\n# Create dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=train_config['batch_size'], shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=train_config['batch_size'], shuffle=False)\n\n# Train model\ntrain_epoch_metrics, val_epoch_metrics, train_epoch_losses, val_epoch_losses = train_model(model, train_dataloader, val_dataloader, train_config, verbose=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning Curves\nHere we visualise the training and validation loss over each epoch, helping assess the model's learning progress and identify overfitting.","metadata":{}},{"cell_type":"code","source":"def plot_learning_curves(train_epoch_losses, val_epoch_losses):\n    plt.style.use('ggplot')\n    plt.rcParams['text.color'] = '#333333'\n\n    fig, axis = plt.subplots(1, 1, figsize=(10, 6))\n\n    # Plot training and validation loss (NaN is used to offset epochs by 1)\n    axis.plot([np.NaN] + train_epoch_losses, color='#636EFA', marker='o', linestyle='-', linewidth=2, markersize=5, label='Training Loss')\n    axis.plot([np.NaN] + val_epoch_losses,   color='#EFA363', marker='s', linestyle='-', linewidth=2, markersize=5, label='Validation Loss')\n\n    # Adding title, labels and formatting\n    axis.set_title('Training and Validation Loss Over Epochs', fontsize=16)\n    axis.set_xlabel('Epoch', fontsize=14)\n    axis.set_ylabel('Loss', fontsize=14)\n\n    axis.set_ylim(0, 1)\n    \n    axis.legend(fontsize=12)\n    axis.grid(True, which='both', linestyle='--', linewidth=0.5)\n    plt.savefig(\"classical_learning_curve.pdf\")\n    plt.show()\n    \nplot_learning_curves(train_epoch_metrics, val_epoch_metrics)","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Viewing a Sample Prediction\nThis code demonstrates the model's performance on a test sample by displaying the input brain MRI images, the model's predicted mask, and the actual mask (ground truth).","metadata":{}},{"cell_type":"code","source":"def display_test_sample(model, test_input, test_target, device):\n    test_input, test_target = test_input.to(device), test_target.to(device)\n\n    # Obtain the model's prediction\n    test_pred = torch.sigmoid(model(test_input))\n\n    # Process the image and masks for visualization\n    image = test_input.detach().cpu().numpy().squeeze(0)\n    mask_pred = test_pred.detach().cpu().numpy().squeeze(0)\n    mask_target = test_target.detach().cpu().numpy().squeeze(0)\n\n    # Set the plot aesthetics\n    plt.rcParams['figure.facecolor'] = '#171717'\n    plt.rcParams['text.color']       = '#DDDDDD'\n\n    # Display the input image, predicted mask, and target mask\n    display_image_channels(image, save=True, name=\"1\")\n    display_mask_channels_as_rgb(mask_pred, title='Predicted Mask Channels as RGB', save=True, name=\"2\")\n    display_mask_channels_as_rgb(mask_target, title='Ground Truth as RGB',  save=True, name=\"3\")\n    \n\n# Set so model and these images are on the same device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n# Get an image from the validation dataset that the model hasn't been trained on\ntest_input, test_target = next(test_input_iterator)\n\ndisplay_test_sample(model, test_input, test_target, device)","metadata":{"_kg_hide-input":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}